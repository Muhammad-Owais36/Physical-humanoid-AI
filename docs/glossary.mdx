---
sidebar_position: 11
---

# Glossary

This glossary provides definitions for key terms used throughout the Physical AI & Humanoid Robotics documentation.

## A

**Action**: In ROS 2, a communication primitive for long-running tasks that provides feedback during execution and results upon completion. Actions are ideal for robot behaviors like navigation and manipulation.

**AI-Robot Brain**: Module 3 of the curriculum focusing on advanced perception and training using NVIDIA Isaac technologies, including Isaac Sim for photorealistic simulation and Isaac ROS for hardware-accelerated navigation.

**Articulated Robot**: A robot with multiple rigid segments connected by joints, allowing for complex movement patterns. Humanoid robots are a type of articulated robot.

## B

**Balance Controller**: A control system that maintains the stability of a humanoid robot by adjusting joint positions and center of mass to prevent falls.

**Behavior Tree**: A hierarchical structure used in robotics for organizing and controlling complex behaviors. Behavior trees are commonly used in navigation systems for decision-making.

**Bipedal Locomotion**: The act of walking on two legs, which is the primary mode of movement for humanoid robots.

## C

**Capstone**: The culminating project that integrates all four modules (ROS 2, Digital Twin, AI-Robot Brain, and Vision-Language-Action) to create an autonomous humanoid robot capable of performing complex tasks.

**Center of Mass (COM)**: The point in a robot where the total mass of the body is considered to be concentrated. Maintaining COM stability is crucial for humanoid balance.

**Citation Integrity**: A principle from the project constitution requiring all technical claims to be verified with official documentation and cited using APA format.

**Communication Primitives**: The fundamental ways ROS 2 nodes communicate: Nodes, Topics, Services, and Actions, which function as the "nervous system" of robotic systems.

**Controller**: A software component that processes sensor data and generates commands to achieve desired robot behaviors, such as maintaining balance or executing trajectories.

**Convolutional Neural Network (CNN)**: A type of deep learning network particularly effective for image and video processing tasks in robotic perception.

## D

**Deep Learning**: A subset of machine learning using neural networks with multiple layers, essential for advanced perception in robotics.

**Digital Twin**: Module 2 of the curriculum focusing on physics simulation and environment building using Gazebo and Unity for creating virtual replicas of physical robots.

**Divergent Component of Motion (DCM)**: A concept used in humanoid locomotion control to plan stable walking patterns by considering the dynamics of the center of mass.

**Domain Randomization**: A technique in simulation where environment parameters (lighting, textures, physics) are randomly varied to improve the transfer of learned behaviors from simulation to reality.

**Dynamics**: The study of forces and torques that cause motion, crucial for understanding and controlling robot movement.

## E

**Embodiment**: The concept of AI systems having a physical form and interacting with the real world through sensors and actuators.

**End-Effector**: The tool or device at the end of a robot arm, such as a gripper or tool, used for manipulation tasks.

**Error Recovery System**: A system component that detects and handles errors in natural language processing and robotic task execution.

## F

**Footstep Planning**: The process of determining the sequence and placement of foot positions for stable bipedal locomotion.

**Forward Kinematics**: The process of calculating the position and orientation of the robot's end-effector based on the joint angles.

**Fused Multiply-Add (FMA)**: A hardware operation that performs multiplication and addition in a single step, commonly used in AI accelerators.

## G

**Gazebo**: A 3D simulation environment that provides accurate physics simulation, high-quality graphics, and convenient programmatic interfaces for robotics development.

**General-Purpose GPU (GPGPU)**: The use of graphics processing units for computational tasks beyond graphics rendering, particularly useful for AI and robotics algorithms.

**Glossary**: A collection of definitions for specialized terms used in a particular field or document.

**Graph Neural Network (GNN)**: A type of neural network designed to work with graph-structured data, useful for multi-robot systems and spatial reasoning.

## H

**Hardware-in-the-Loop (HIL)**: A testing methodology that involves running a simulation with actual hardware components to validate system behavior.

**Humanoid Robot**: A robot designed to resemble and move like a human, typically with a head, torso, two arms, and two legs.

**Human-Robot Interaction (HRI)**: The study and practice of designing robots that can interact safely and effectively with humans.

## I

**Inverse Kinematics**: The process of calculating the joint angles required to achieve a desired position and orientation of the robot's end-effector.

**Isaac ROS**: NVIDIA's collection of hardware-accelerated perception and navigation packages for ROS 2, leveraging GPU computing capabilities.

**Isaac Sim**: NVIDIA's robotics simulation platform built on Omniverse, designed for developing, testing, and training AI-powered robots.

**Iterative Learning Control (ILC)**: A control method that improves performance by learning from repeated execution of the same task.

## J

**Joint**: A connection between two rigid bodies that allows for specific types of relative motion, such as rotation or translation.

**Joint Space**: The space defined by the robot's joint angles, as opposed to Cartesian space which is defined by position coordinates.

**Jumping Distance**: A metric used in humanoid locomotion to measure the distance between consecutive foot placements.

## K

**Kinematics**: The study of motion without considering the forces that cause it, focusing on position, velocity, and acceleration relationships.

**Kinodynamic Planning**: Motion planning that considers both kinematic constraints and dynamic limitations of the robot.

**Kubernetes**: An open-source system for automating deployment, scaling, and management of containerized applications, sometimes used for robotics deployment.

## L

**Legged Locomotion**: The ability of robots with legs to move through their environment, particularly challenging for humanoid robots.

**LiDAR**: Light Detection and Ranging, a sensing technology that uses laser pulses to measure distances and create 3D maps.

**Localization**: The process of determining the robot's position and orientation within a known map of the environment.

**Long Short-Term Memory (LSTM)**: A type of recurrent neural network that can learn long-term dependencies, useful for sequential robot behaviors.

## M

**Manipulation**: The ability of a robot to purposefully change the position or state of objects in its environment using its end-effectors.

**Markov Decision Process (MDP)**: A mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.

**Middleware**: Software that provides common services and capabilities to applications beyond what's offered by the operating system, such as ROS 2.

**Modularity in System Design**: A principle requiring physical AI systems to be designed with modular components that can operate independently.

## N

**Navigation**: The ability of a robot to move from one location to another while avoiding obstacles and maintaining stability.

**Neural Radiance Fields (NeRF)**: A technique for synthesizing novel views of complex 3D scenes from a sparse set of 2D images.

**Newton-Euler Algorithm**: A method for computing the inverse dynamics of a robot, calculating the forces and torques required for a given motion.

## O

**Occupancy Grid**: A probabilistic representation of space that divides the environment into discrete cells, each containing the probability of being occupied.

**OpenAI Whisper**: A state-of-the-art automatic speech recognition system that converts speech to text, used in the Vision-Language-Action module.

**Operational Space Control**: A control framework that allows specifying desired motion and forces directly in the task space rather than joint space.

## P

**Path Planning**: The process of determining a sequence of configurations that move the robot from a start state to a goal state while avoiding obstacles.

**Perception**: The ability of a robot to interpret sensory information about its environment and internal state.

**Physical AI**: The convergence of artificial intelligence with physical robotic systems, requiring AI systems to navigate the complexities of the real world.

**Point Cloud**: A collection of data points in 3D space, typically generated by 3D scanners or LiDAR sensors.

**Proportional-Integral-Derivative (PID) Controller**: A control loop mechanism widely used in robotics for maintaining desired states.

## Q

**Quaternion**: A mathematical construct used to represent rotations and orientations in 3D space, avoiding the gimbal lock problem of Euler angles.

**Quality of Service (QoS)**: A set of policies in ROS 2 that define how messages are delivered, including reliability, durability, and history.

## R

**Recurrent Neural Network (RNN)**: A type of neural network designed to recognize patterns in sequences of data, useful for temporal robot behaviors.

**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards or penalties.

**Reproducibility**: A principle requiring methods and experiments in simulation environments to be traceable and explainable for students and researchers.

**Rigidity**: The property of maintaining a fixed shape and size under applied forces, important for robot structural design.

**Robot Operating System 2 (ROS 2)**: The middleware that connects all components of the robotic system, acting as the "nervous system" for communication between sensors, controllers, and AI systems.

**ROS 2 Client Library for Python (rclpy)**: The Python client library for ROS 2, providing Python bindings for ROS 2's communication primitives.

## S

**Sensor Fusion**: The process of combining data from multiple sensors to achieve better accuracy and reliability than could be achieved by using a single sensor.

**Simulation-to-Reality Gap**: The difference in performance between a robot system in simulation versus in the real world, which must be bridged for successful deployment.

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**State Estimation**: The process of determining the internal state of a robot from noisy sensor measurements and control inputs.

**Stereopsis**: The perception of depth and 3D structure obtained on the basis of binocular vision, used in stereo vision systems.

**Synchronization**: The coordination of events to operate in unison, critical for real-time robotic systems.

## T

**Tensor Processing Unit (TPU)**: A specialized integrated circuit developed by Google for machine learning workloads, relevant to AI-accelerated robotics.

**Trajectory**: A time-parameterized path that specifies the position, velocity, and acceleration of a robot over time.

**Transform (TF)**: A package in ROS that keeps track of multiple coordinate frames over time, essential for spatial relationships in robotics.

**Trust Region Policy Optimization (TRPO)**: A reinforcement learning algorithm that ensures stable policy updates, useful for robot learning.

## U

**Unified Robot Description Format (URDF)**: An XML-based format for representing a robot model, including links, joints, and other elements.

**Unity**: A real-time 3D development platform used for creating high-fidelity visualizations and human-robot interaction scenarios in the Digital Twin module.

**Universal Scene Description (USD)**: A 3D scene representation and composition format developed by Pixar, used in Isaac Sim.

## V

**Vision-Language-Action (VLA)**: Module 4 of the curriculum focusing on integrating LLMs with robotics for cognitive planning and natural interaction through voice commands.

**Visual Servoing**: A control method that uses visual feedback to control the motion of a robot.

**Volumetric Representation**: A 3D representation that encodes the interior of objects, useful for collision detection and manipulation planning.

## W

**Whole-Body Control**: A control approach that considers the entire robot system when generating control commands, optimizing for multiple objectives simultaneously.

**Whisper**: OpenAI's automatic speech recognition system that converts speech to text, forming the basis for voice command processing in the VLA module.

## X

**Xacro**: An XML macro language for generating URDF files, allowing for more readable and maintainable robot descriptions.

## Y

**Yaw**: The rotation of a robot around its vertical axis, one of the three rotational degrees of freedom.

## Z

**Zero-Moment Point (ZMP)**: A concept in bipedal robotics that defines the point on the ground where the net moment of the ground reaction force is zero, crucial for balance control.

**Z-Buffer**: A component of 3D graphics rendering that handles depth information, important in simulation environments.